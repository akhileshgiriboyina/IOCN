{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPvSm/aJnnuL6o/xpBQemDF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akhileshgiriboyina/IOCN/blob/main/ensemblemethods2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ENSEMBLE OF IOC-NN --> **BOOSTING + GATING**"
      ],
      "metadata": {
        "id": "2sWyQ1pM6Mn6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "--> **SETUP**\n",
        "\n",
        "Each IOC-NN is trained individually. The first\n",
        "model is trained on the whole data, and the consecutive models are trained with\n",
        "exaggerated data on the samples on which the previous model performs poorly. For\n",
        "bootstrapping, we use a simple re-weighting mechanism as in [10]. A gating net\u0002work is then trained over the ensemble of IOC-NNs. The weights of the individual\n",
        "networks are frozen while training the gating network.\n",
        "\n",
        "-->**Training Boosted ensembles** \n",
        "\n",
        "The lower training accuracy of IOC-NNs makes them\n",
        "suitable for boosting (while the training accuracy saturates in non-convex counterparts).\n",
        "For bootstrapping, we use a simple re-weighting mechanism as in [10]. We train three\n",
        "experts for each experiment. The gating network is a regular neural network, which is\n",
        "a shallow version of the actual experts. We train an MLP with only one hidden layer,\n",
        "a four-layer fully convolutional network, and a DenseNet with two dense-blocks as\n",
        "the gate for the three respective architectures. We report the accuracy of the ensemble\n",
        "trained in this fashion as well as the accuracy if we would have used an oracle instead\n",
        "of the gating network.\n"
      ],
      "metadata": {
        "id": "eDTVpj7Z635x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision import models\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "dqK1_mfF6iOd"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the CIFAR-10 dataset\n",
        "train_dataset = CIFAR10(root='./data', train=True, download=True, transform=ToTensor())\n",
        "test_dataset = CIFAR10(root='./data', train=False, download=True, transform=ToTensor())\n",
        "\n",
        "# Define the dataloaders\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Define the number of individual models (IOC-NNs) in the ensemble\n",
        "num_ensembles = 3\n",
        "\n",
        "# Create a list to store the outputs of individual models\n",
        "model_outputs = []"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PknJvTNDUFI",
        "outputId": "b02f2318-bc01-474d-a21d-2bacf24a4c6c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:01<00:00, 93232434.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Define the MLP model architecture\n",
        "# class MLP(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(MLP, self).__init__()\n",
        "#         self.flatten = nn.Flatten()\n",
        "#         self.fc1 = nn.Linear(32 * 32 * 3, 128)\n",
        "#         self.relu = nn.ReLU()\n",
        "#         self.fc2 = nn.Linear(128, 64)\n",
        "#         self.fc3 = nn.Linear(64, 10)\n",
        "    \n",
        "#     def forward(self, x):\n",
        "#         x = self.flatten(x)\n",
        "#         x = self.relu(self.fc1(x))\n",
        "#         x = self.relu(self.fc2(x))\n",
        "#         x = self.fc3(x)\n",
        "#         return x"
      ],
      "metadata": {
        "id": "mhtPY7HoS4ex"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OzMUklDY9P_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, InputDim, OutputDim):\n",
        "        super(MLP, self).__init__()\n",
        "        self.InputDim = InputDim\n",
        "        self.OutputDim = OutputDim\n",
        "        self.Linear1 = nn.Linear(InputDim, 800)\n",
        "        self.Linear2 = nn.Linear(800, 800)\n",
        "        self.Linear3 = nn.Linear(800, 800)\n",
        "        self.Linear4 = nn.Linear(800, self.OutputDim)\n",
        "        self.ActFunc = nn.functional.relu\n",
        "        self.batch = nn.BatchNorm1d(800)\n",
        "        # self.SftMax = nn.functional.softmax\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.ActFunc(self.batch(self.Linear1(x)))\n",
        "        x = self.ActFunc(self.batch(self.Linear2(x)))\n",
        "        x = self.ActFunc(self.batch(self.Linear3(x)))\n",
        "\n",
        "        output = self.Linear4(x)\n",
        "        \n",
        "        return output\n",
        "    \n",
        "    # def InitWeights(self):\n",
        "    #     torch.nn.init.uniform_(self.Linear1.weight,-0.5, 0.5)\n",
        "    #     torch.nn.init.uniform_(self.Linear2.weight,-0.5, 0.5)"
      ],
      "metadata": {
        "id": "e-T6s3Xwx_eq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of the MLP model\n",
        "normal_MLP_model = MLP(3072, 10)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(normal_MLP_model.parameters(), lr=0.0001)"
      ],
      "metadata": {
        "id": "9LDF-5MnT2wz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model,loss_func,optimizer):\n",
        "  # Training loop\n",
        "  num_epochs = 10\n",
        "  for epoch in range(num_epochs):\n",
        "      model.train()\n",
        "      for images, labels in train_dataloader:\n",
        "          images = images.reshape(images.shape[0],-1)\n",
        "\n",
        "          # Zero the gradients\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          # Forward pass\n",
        "          outputs = model(images)\n",
        "          loss = loss_func(outputs, labels)\n",
        "\n",
        "          # Backward pass and optimization\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "      # Print the loss after each epoch\n",
        "      print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}\")"
      ],
      "metadata": {
        "id": "kBTwwl0gT4Cw"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model):\n",
        "  model.eval()\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  with torch.no_grad():\n",
        "      for images, labels in test_dataloader:\n",
        "          outputs = model(images)\n",
        "          _, predicted = torch.max(outputs.data, 1)\n",
        "          total += labels.size(0)\n",
        "          correct += (predicted == labels).sum().item()\n",
        "\n",
        "  # Calculate the accuracy\n",
        "  accuracy = correct / total\n",
        "  print(f\"\\nTest Accuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "JJmdifEnUS8n"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(normal_MLP_model,loss_func,optimizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "xnnbXL37UxCL",
        "outputId": "b000a324-4701-4795-e104-fab00fc90f13"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 1.3432490825653076\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-69-b99032f0a685>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormal_MLP_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-67-3476f4acf37f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, loss_func, optimizer)\u001b[0m\n\u001b[1;32m     16\u001b[0m           \u001b[0;31m# Backward pass and optimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m           \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m           \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m       \u001b[0;31m# Print the loss after each epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    278\u001b[0m                                                f\"but got {result}.\")\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'differentiable'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    139\u001b[0m                 state_steps)\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             adam(\n\u001b[0m\u001b[1;32m    142\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m     func(params,\n\u001b[0m\u001b[1;32m    282\u001b[0m          \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m          \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    389\u001b[0m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction2_sqrt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction2_sqrt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m             \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(normal_MLP_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYdBkot7VJgX",
        "outputId": "988bc799-1765-4f79-db2a-3fa0ade0767e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test Accuracy: 45.08%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class WeightedDataset(Dataset):\n",
        "    def __init__(self, data, targets, weights):\n",
        "        self.data = data\n",
        "        self.targets = targets\n",
        "        self.weights = weights\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image, target, weight = self.data[index], self.targets[index], self.weights[index]\n",
        "        return image, target, weight\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ],
      "metadata": {
        "id": "XmFL5tWgojS6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a list to store the outputs of each model\n",
        "model_outputs = []\n",
        "\n",
        "# Create lists to store the accuracy and loss of each model\n",
        "model_accuracies = []\n",
        "model_losses = []\n",
        "ensembles = []\n",
        "\n",
        "# Define the number of ensemble models\n",
        "num_models = 3\n",
        "\n",
        "# Initialize the sample weights for AdaBoost\n",
        "sample_weights = np.ones(len(train_dataset)) / len(train_dataset)\n",
        "model_errors = []\n",
        "# final_sample_wts = [[]]\n",
        "final_outputs = []\n",
        "\n",
        "# Train the ensemble models\n",
        "for model_num in range(num_models):\n",
        "    print(\"\\nTraining model\\t\",model_num+1)\n",
        "\n",
        "    # Create an instance of the MLP model\n",
        "    model = MLP(3072,10).to(device)\n",
        "    # model = MLP(3072,10)\n",
        "    # Define the loss function and optimizer\n",
        "    loss_func = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "    \n",
        "\n",
        "    # Define the batch size\n",
        "    batch_size = 32  # Replace with your desired batch size\n",
        "\n",
        "    # Convert sample_weights list to a NumPy array\n",
        "    sample_weights = np.array(sample_weights)\n",
        "    # print(len(sample_weights))\n",
        "    # Convert train_dataset.targets list to a NumPy array\n",
        "    train_targets = np.array(train_dataset.targets)\n",
        "\n",
        "    # Convert NumPy arrays to PyTorch tensors\n",
        "    train_data_tensor = torch.from_numpy(train_dataset.data)\n",
        "    train_targets_tensor = torch.from_numpy(train_targets)\n",
        "    sample_weights_tensor = torch.from_numpy(sample_weights)\n",
        "\n",
        "    # Move tensors to the desired device\n",
        "    # train_data_tensor = train_data_tensor.to(device)\n",
        "    # train_targets_tensor = train_targets_tensor.to(device)\n",
        "    # sample_weights_tensor = sample_weights_tensor.to(device)\n",
        "\n",
        "    # Reset the training dataset with updated sample weights\n",
        "    train_dataset_weighted = WeightedDataset(train_data_tensor, train_targets_tensor, sample_weights_tensor)\n",
        "\n",
        "    # print(len(train_dataset_weighted))\n",
        "\n",
        "    # Create a new data loader with updated sample weights\n",
        "    train_dataloader = torch.utils.data.DataLoader(train_dataset_weighted, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # print(len(train_dataloader))\n",
        "\n",
        "    # Training loop\n",
        "    num_epochs = 5\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        # Create a list to store the predictions of the current model\n",
        "        model_predictions = []\n",
        "\n",
        "\n",
        "        for images, labels, weights in train_dataloader:\n",
        "            # Convert the input tensor to the same data type as the model's weight tensors\n",
        "            images = images.reshape(images.shape[0],-1)\n",
        "            images = images.to(device,dtype=torch.float)\n",
        "\n",
        "            labels = labels.to(device)\n",
        "            weights = weights.to(device)\n",
        "            \n",
        "            # Zero the gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            loss = (weights * loss_func(outputs, labels)).mean()\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # print(len(outputs.data))\n",
        "\n",
        "            # Store the predictions of the current model\n",
        "            _, predictions = torch.max(outputs.data, 1)\n",
        "            model_predictions.extend(predictions.tolist())\n",
        "\n",
        "            # Compute accuracy and update metrics\n",
        "            running_loss += loss.item()\n",
        "            total += labels.size(0)\n",
        "            correct += (predictions == labels).sum().item()\n",
        "\n",
        "        # Calculate accuracy and loss for the current epoch\n",
        "        epoch_loss = running_loss / len(train_dataloader)\n",
        "        epoch_accuracy = correct / total\n",
        "\n",
        "        # Print epoch-level accuracy and loss\n",
        "        print(\"Epoch:\\t\",epoch+1, \"\\t\\tAccuracy:\\t\",f\"{epoch_accuracy:.4f}\")\n",
        "\n",
        "    # Store the accuracy and loss of the current model\n",
        "    model_accuracies.append(epoch_accuracy)\n",
        "    model_losses.append(epoch_loss)\n",
        "\n",
        "    # Update the weights based on the weighted error\n",
        "    # print(model_predictions[6:])\n",
        "    # print(train_dataset.targets[6:])\n",
        "    # print(np.array(model_predictions).shape)\n",
        "    # print(train_dataset_weighted.targets.numpy().shape)\n",
        "    weighted_error = np.sum(sample_weights * np.not_equal(np.array(model_predictions), train_dataset_weighted.targets.numpy()))\n",
        "    model_errors.append(weighted_error)\n",
        "    weights_update = np.log((1 - weighted_error) / (weighted_error + 1e-10)) / 2\n",
        "    sample_weights *= np.exp(weights_update * np.not_equal(np.array(model_predictions), train_dataset_weighted.targets.numpy()))\n",
        "    sample_weights /= (np.sum(sample_weights) + 1e-10)\n",
        "    final_sample_wts = sample_weights\n",
        "\n",
        "    # Store the predictions of the current model in the list of model outputs\n",
        "    model_outputs.append(model_predictions)\n",
        "    final_outputs.append(model_outputs)\n",
        "    ensembles.append(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        },
        "id": "wbA6qyiRD_sk",
        "outputId": "2fb96357-2684-4f21-8009-af6940e27c41"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training model\t 1\n",
            "Epoch:\t 1 \t\tAccuracy:\t 0.4297\n",
            "Epoch:\t 2 \t\tAccuracy:\t 0.5095\n",
            "Epoch:\t 3 \t\tAccuracy:\t 0.5480\n",
            "Epoch:\t 4 \t\tAccuracy:\t 0.5834\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-cee9d4a0cdc5>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;31m# Backward pass and optimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print(final_sample_wts)\n",
        "print(len(model_outputs[2]))\n",
        "print(model_errors)"
      ],
      "metadata": {
        "id": "G_RcxYQfEA2L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5d62266-f755-4645-f740-fb2fc7d0abc0"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50000\n",
            "[0.89978, 0.8971047846955986, 0.8978786673431151]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model_outputs[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "yhQHKW252xFs",
        "outputId": "a2fb6606-e3a3-40a0-93e5-c7a7e7c75eb4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-5b9318bd0f51>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model_outputs' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "errors = np.array(model_errors)\n",
        "res = []\n",
        "for i in errors:\n",
        "  res.append(0.5*np.log((1 - i)/i))\n",
        "res"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCsTg8rtpWNW",
        "outputId": "1eb1b554-c3bc-47ee-aee0-6a64325b182c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[-1.0973912597334377, -1.0827307644029056, -1.0869366513286594]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GatingNetwork(nn.Module):\n",
        "    def __init__(self, input_size, num_models):\n",
        "        super(GatingNetwork, self).__init__()\n",
        "        self.fc = nn.Linear(input_size, num_models)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc(x)\n",
        "        x = self.softmax(x)\n",
        "        return x\n",
        "\n",
        "# Example usage\n",
        "input_size = num_models * 2  # Assuming each model's prediction and error/significance are concatenated\n",
        "num_models = 3  # Number of ensemble models\n",
        "# gating_model = GatingNetwork(input_size, num_models)"
      ],
      "metadata": {
        "id": "hS8Ov_bVzUQ-"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "significance_tensor = torch.tensor(res)\n",
        "predictions_tensor = torch.tensor(model_outputs)\n",
        "gating_output = torch.sign(torch.sum(significance_tensor.unsqueeze(1) * predictions_tensor, dim=0))"
      ],
      "metadata": {
        "id": "hTd-cXZNERcY"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "go_list = gating_output.tolist()\n",
        "print(go_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MB5NrU9J2BKm",
        "outputId": "49f6c12e-8ecc-4b49-f299-630ce80f0986"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the ensemble on the testing data\n",
        "ensemble_predictions = np.argmax(gating_network.predict(model_outputs), axis=1)\n",
        "ensemble_accuracy = np.mean(ensemble_predictions == y_train)\n",
        "\n",
        "print(\"Ensemble accuracy:\", ensemble_accuracy)"
      ],
      "metadata": {
        "id": "24mKmtqLEU4C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}